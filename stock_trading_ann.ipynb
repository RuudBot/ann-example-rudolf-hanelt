{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "# import DatasetCreation from Data preparation in order to load training data\n",
    "sys.path.insert(0, '.../src/Ann/DatasetCreation/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE COMPUTABLE FOR CPU (no nvidia card available)\n",
    "# TODO: MICROSOFT AZURE CONFIGURATION\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "num_cores = 8\n",
    "\n",
    "num_CPU = 1\n",
    "num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTION: prevents data bias toward upwards market trend\n",
    "def balance_short_long(targets_train, training_indizes):\n",
    "    \n",
    "    n_more_longs_than_shorts = sum(targets_train[training_indizes])\n",
    "    algebraic_sign = 0\n",
    "    if  n_more_longs_than_shorts > 0:\n",
    "        algebraic_sign = 1\n",
    "    else:\n",
    "        algebraic_sign = -1\n",
    "    i = 0\n",
    "    \n",
    "    # for loop not optimal (vectorization is), but only a couple thousand operations each run\n",
    "    while  n_more_longs_than_shorts*algebraic_sign > 0:\n",
    "        random_index = np.random.randint(0, len(training_indizes))\n",
    "        if targets_train[training_indizes[random_index]]*algebraic_sign > 0:\n",
    "            training_indizes = np.delete(training_indizes, random_index)\n",
    "            i = i+1\n",
    "             n_more_longs_than_shorts =  n_more_longs_than_shorts - algebraic_sign\n",
    "\n",
    "    print(sum(targets_train[training_indizes]))\n",
    "    return training_indizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRAINING DATASET\n",
    "import time \n",
    "import ann3xTrainingDatasetService\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "service = ann3xTrainingDatasetService.ann3xTrainingDatasetService()\n",
    "\n",
    "# not sampling with randomized 0.3/0.7 datasets, because of strong korrelation of different stocks through beta (overall market movement)\n",
    "# solution: test on future market activity\n",
    "\n",
    "dly_cnn_input, \\\n",
    "dly_mlp_input, \\\n",
    "wkly_cnn_input, \\\n",
    "wkly_mlp_input, \\\n",
    "dly_indicator_cnn_input, \\\n",
    "wkly_indicator_cnn_input, \\\n",
    "targets, changes = service.get_inputs_and_target(\n",
    "            year_array = [2013,2014,2015,2016,2017,2018],\n",
    "            quarter_array = [1,2,3,4],\n",
    "            time_steps_dly = 100,\n",
    "            time_steps_wkly = 50,\n",
    "            indicator_length_dly = 20,\n",
    "            indicator_length_wkly = 20)\n",
    "\n",
    "\n",
    "print(\"Time to load training data in seconds:\")\n",
    "print(time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TEST DATASET\n",
    "import time\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "service = ann3xTrainingDatasetService.ann3xTrainingDatasetService()\n",
    "\n",
    "dly_cnn_input_test, \\\n",
    "dly_mlp_input_test, \\\n",
    "wkly_cnn_input_test, \\\n",
    "wkly_mlp_input_test, \\\n",
    "dly_indicator_cnn_input_test, \\\n",
    "wkly_indicator_cnn_input_test, \\\n",
    "targets_test, changes_test = service.get_inputs_and_target(\n",
    "            year_array = [2019],\n",
    "            quarter_array = [1,2,3,4],\n",
    "            time_steps_dly = 100,\n",
    "            time_steps_wkly = 50,\n",
    "            indicator_length_dly = 20,\n",
    "            indicator_length_wkly = 20)\n",
    "\n",
    "\n",
    "print(\"Time to load testing data in seconds:\")\n",
    "print(time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOMIZED LOSS FUNCTION\n",
    "def trading_loss(y_true, y_pred):\n",
    "    # Squared Mean Error with 4 times factored error for trading loss\n",
    "    # loss function still up for discussion, because MSE is not optimal for binary classification (long-short)\n",
    "    # Nevertheless it was the best performing loss function in current constellation\n",
    "    \n",
    "    # check if prediction resulted in trading loss\n",
    "    wrong_pred_factor = (tf.cast((y_true*y_pred)<0, 'float32')*3)\n",
    "    \n",
    "    loss = (0.5-K.mean(y_pred * y_true + (y_pred * y_true)*wrong_pred_factor, axis=-1))\n",
    "    return (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN MODEL CLASS\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "def create_mlp(dim, regress=False):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=dim, activation=\"tanh\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128,activation=\"tanh\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(64,activation=\"tanh\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32,activation=\"tanh\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation=\"tanh\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(8, activation=\"tanh\"))\n",
    "    model.add(Dense(3, activation=\"tanh\"))\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"tanh\")))\n",
    "    return model\n",
    "\n",
    "def create_cnn(n_days, n_features, filters, kernel_size, regress=False):\n",
    "    # CNN with 1D convolution for time series\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    if n_days > 50:\n",
    "        model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=(n_days,n_features)))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(MaxPooling1D(2))\n",
    "    else:\n",
    "        model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=(n_days,n_features)))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        \n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Flatten())\n",
    "    if (n_features > 5):\n",
    "        model.add(Dense(75, activation='tanh'))\n",
    "        \n",
    "    # Fully connected layers\n",
    "    model.add(Dense(36, activation='tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(3, activation='tanh'))\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"tanh\"))\n",
    "    # return the CNN\n",
    "    return model\n",
    "\n",
    "def create_lstm(n_days, n_features,  regress=False):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=False, dropout_U = 0.1, dropout_W = 0.1, input_shape=(n_days,n_features)))\n",
    "    model.add(Dense(16, activation='tanh'))\n",
    "    model.add(Dense(8, activation='tanh'))\n",
    "    model.add(Dense(3, activation='tanh'))\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"tanh\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import concatenate\n",
    "import keras\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "import os\n",
    "\n",
    "\n",
    "# create the MLP and CNN models\n",
    "mlp = create_mlp(dly_mlp_input_train.shape[1], regress=False)\n",
    "mlp_wkly = create_mlp(wkly_mlp_input_train.shape[1], regress=False)\n",
    "cnn = create_cnn(dly_cnn_input_train.shape[1], dly_cnn_input_train.shape[2], 256 , 3, regress=False)\n",
    "cnn_wkly = create_cnn(wkly_cnn_input_train.shape[1], wkly_cnn_input_train.shape[2], 256 , 3, regress=False)\n",
    "\n",
    "# create the input to our final set of layers as the *output* of the MLP and CNN models\n",
    "combinedInput = concatenate([cnn.output, mlp.output, mlp_wkly.output, cnn_wkly.output])\n",
    "\n",
    "# our final FC layer\n",
    "x = Dense(18, activation=\"tanh\")(combinedInput)\n",
    "x = Dense(9, activation=\"tanh\")(x)\n",
    "x = Dense(3, activation=\"tanh\")(x)\n",
    "x = Dense(1, activation=\"tanh\")(x)\n",
    "\n",
    "model = Model(inputs=[cnn.input, mlp.input, mlp_wkly.input, cnn_wkly.input], outputs=x)\n",
    "\n",
    "model.compile(  loss = trading_loss, optimizer='SGD', metrics = ['accuracy'])\n",
    "\n",
    "print(\"[INFO] training model...\")\n",
    "\n",
    "epochs = 1000\n",
    "history = model.fit(\n",
    "            [dly_cnn_input_train, dly_mlp_input_train, wkly_mlp_input_train, wkly_cnn_input_train], targets_train,\n",
    "            validation_data=([dly_cnn_input_test, dly_mlp_input_test, wkly_mlp_input_test, wkly_cnn_input_test], targets_test),\n",
    "            epochs=epochs, batch_size=32)\n",
    "i= epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT MODEL OUTPUT FOR ANALYSIS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "predictions = model.predict([dly_cnn_input_test, dly_mlp_input_test, wkly_mlp_input_test, wkly_cnn_input_test])\n",
    "\n",
    "predictions = predictions.flatten()\n",
    "actual_changes = changes_test\n",
    "\n",
    "Y = np.arange(len(actual_changes))\n",
    "\n",
    "print('---ANN output---')\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(Y, predictions, width=1, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL \n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pickle\n",
    "import time\n",
    "import pymongo\n",
    "\n",
    "def save_model_to_db(model, client, db, dbconnection, model_name):\n",
    "    import pickle\n",
    "    import time\n",
    "    import pymongo\n",
    "    #pickling the model\n",
    "    pickled_model = pickle.dumps(model)\n",
    "    \n",
    "    #saving model to mongoDB\n",
    "    # creating connection\n",
    "    myclient = pymongo.MongoClient(client)\n",
    "    \n",
    "    #creating database in mongodb\n",
    "    mydb = myclient[db]\n",
    "    \n",
    "    #creating collection\n",
    "    mycon = mydb[dbconnection]\n",
    "    info = mycon.insert_one({model_name: pickled_model, 'name': model_name, 'created_time':time.time()})\n",
    "    print(info.inserted_id, ' saved with this id successfully!')\n",
    "    \n",
    "    details = {\n",
    "        'inserted_id':info.inserted_id,\n",
    "        'model_name':model_name,\n",
    "        'created_time':time.time()\n",
    "    }\n",
    "    \n",
    "    return details\n",
    "\n",
    "def load_saved_model_from_db(model_name, client, db, dbconnection):\n",
    "    json_data = {}\n",
    "    \n",
    "    #saving model to mongoDB\n",
    "    # creating connection\n",
    "    myclient = pymongo.MongoClient(client)\n",
    "    \n",
    "    #creating database in mongodb\n",
    "    mydb = myclient[db]\n",
    "    \n",
    "    #creating collection\n",
    "    mycon = mydb[dbconnection]\n",
    "    data = mycon.find({'name': model_name})\n",
    "    \n",
    "    \n",
    "    for i in data:\n",
    "        json_data = i\n",
    "    #fetching model from db\n",
    "    pickled_model = json_data[model_name]\n",
    "    \n",
    "    return pickle.loads(pickled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "\n",
    "# add customized loss to keras losses, in order to be able to save model in MongoDB\n",
    "keras.losses.trading_loss = trading_loss\n",
    "\n",
    "#saving model to mongo\n",
    "details = save_model_to_db(model = model, client ='mongodb://localhost:27017/', db = 'stockTraderDB', \n",
    "                 dbconnection = 'models', model_name = 'TripleXAnnModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
